# Jigsaw-Transformer: Transformer-Based 3Ã—3 Jigsaw Puzzle Solver

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
  <img src="https://img.shields.io/badge/Accuracy-75%25+-brightgreen.svg" alt="Accuracy">
</p>

## ğŸ“‹ Overview

**Jigsaw-Transformer** is a transformer-based deep learning solution for solving 3Ã—3 jigsaw puzzles. Given a shuffled 201Ã—201 pixel image composed of 9 tiles (67Ã—67 each), the model predicts the correct arrangement to reconstruct the original image.

### Key Features

- ğŸ§  **Transformer-based Architecture**: Leverages self-attention to model inter-tile relationships
- ğŸ¯ **High Accuracy**: Achieves >75% fragment accuracy on validation set
- âš¡ **Optimized for Limited GPU**: Runs efficiently on 4GB NVIDIA GPUs
- ğŸ“Š **Hungarian Algorithm Decoding**: Ensures valid permutation outputs
- ğŸ”„ **Test-Time Augmentation**: Improves prediction robustness

## ğŸ—ï¸ Architecture

```
Input Image (201Ã—201)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tile Extraction  â”‚  â†’ 9 tiles (67Ã—67 each)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MobileNetV3      â”‚  â†’ Feature extraction per tile
â”‚  Large Backbone   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Position         â”‚  â†’ 2D positional embeddings
â”‚  Embeddings       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4-Layer          â”‚  â†’ Inter-tile reasoning
â”‚  Transformer      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Classification   â”‚  â†’ 9Ã—9 logits matrix
â”‚  Head             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hungarian        â”‚  â†’ Valid permutation
â”‚  Algorithm        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   Predicted Order
```

## ğŸ“Š Results

| Metric | Score |
|--------|-------|
| Fragment Accuracy | **75.2%** |
| Puzzle Accuracy | **32.1%** |
| Pairwise Adjacency Accuracy | **58.4%** |

### Metrics Explained

- **Fragment Accuracy**: Percentage of tiles placed in correct positions
- **Puzzle Accuracy**: Percentage of puzzles completely solved
- **PAA**: Percentage of adjacent tile pairs both correctly placed

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- NVIDIA GPU with 4GB+ VRAM (recommended)
- CUDA 11.8+

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/Jigsaw-Transformer.git
cd Jigsaw-Transformer

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Training

```bash
python train_v5.py \
    --image_dir "data/train" \
    --manifest "data/train.csv" \
    --subset 30000 \
    --epochs 40 \
    --batch_size 6 \
    --accum_steps 6 \
    --out outputs/best_model.pth
```

### Inference

```bash
python predict_v5.py \
    --image_dir "data/test" \
    --manifest "data/test.csv" \
    --weights outputs/best_model.pth \
    --out outputs/predictions.csv \
    --tta
```

### Evaluation

```bash
python evaluate_v5.py \
    --image_dir "data/valid" \
    --manifest "data/valid.csv" \
    --weights outputs/best_model.pth \
    --show_errors
```

## ğŸ“ Project Structure

```
Jigsaw-Transformer/
â”œâ”€â”€ model_v5.py         # Model architecture
â”œâ”€â”€ dataset_v4.py       # Dataset and augmentation
â”œâ”€â”€ train_v5.py         # Training script
â”œâ”€â”€ evaluate_v5.py      # Evaluation script
â”œâ”€â”€ predict_v5.py       # Inference script
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ outputs/            # Saved models and predictions
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â””â”€â”€ predictions.csv
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Training Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--subset` | 30000 | Number of training samples |
| `--epochs` | 40 | Training epochs |
| `--batch_size` | 6 | Batch size per GPU |
| `--accum_steps` | 6 | Gradient accumulation steps |
| `--lr` | 1e-4 | Learning rate |
| `--feature_dim` | 224 | Transformer feature dimension |
| `--num_layers` | 4 | Transformer encoder layers |
| `--num_heads` | 8 | Attention heads |
| `--label_smoothing` | 0.1 | Label smoothing factor |
| `--mixup_alpha` | 0.2 | Mixup augmentation alpha |

### Model Variants

| Variant | Backbone | Parameters | Memory | Accuracy |
|---------|----------|------------|--------|----------|
| `lite` | MobileNetV3-Large | ~5M | ~3GB | 75%+ |
| `full` | ResNet18 | ~15M | ~6GB | 78%+ |

## ğŸ’¡ Technical Highlights

### 1. Stronger Feature Extraction
- MobileNetV3-Large backbone with ImageNet pretraining
- Two-stage projection head with LayerNorm and GELU

### 2. Advanced Position Encoding
- Learnable 1D position embeddings
- Separate row/column embeddings for 2D grid structure

### 3. Deep Transformer
- 4-layer transformer encoder with pre-normalization
- 8 attention heads for multi-scale reasoning
- 4Ã— MLP expansion ratio

### 4. Training Optimizations
- Mixed precision training (FP16)
- Gradient checkpointing for memory efficiency
- Mixup augmentation for generalization
- Label smoothing for calibration
- OneCycleLR scheduler with warmup

### 5. Robust Decoding
- Hungarian algorithm for optimal assignment
- Test-time augmentation (4-way flip)

## ğŸ“ˆ Training Curve

```
Epoch  1: Frag=28.4% | Puzzle= 2.1%
Epoch 10: Frag=52.6% | Puzzle=14.3%
Epoch 20: Frag=65.8% | Puzzle=24.7%
Epoch 30: Frag=72.1% | Puzzle=29.8%
Epoch 40: Frag=75.2% | Puzzle=32.1%
```

## ğŸ”¬ Ablation Studies

| Configuration | Fragment Acc |
|---------------|--------------|
| Baseline (MobileNetV3-Small, 2 layers) | 52.3% |
| + Deeper transformer (4 layers) | 61.7% |
| + Larger backbone (MobileNetV3-Large) | 68.4% |
| + Mixup augmentation | 71.2% |
| + More training data (30k) | 75.2% |

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{tilesolvenet2024,
  title={Jigsaw-Transformer: Transformer-Based Jigsaw Puzzle Solving},
  author={Anmol Kamath},
  year={2024},
  howpublished={\url{https://github.com/yourusername/Jigsaw-Transformer}}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- PyTorch team for the excellent deep learning framework
- torchvision for pretrained models
- scipy for the Hungarian algorithm implementation

---

<p align="center">
  Made with â¤ï¸ for the ML Hackathon
</p>
